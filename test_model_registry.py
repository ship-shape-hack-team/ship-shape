#!/usr/bin/env python3
"""Test quality assessment on kubeflow/model-registry repository."""

import sys
from pathlib import Path
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

print("=" * 80)
print("ğŸ” QUALITY ASSESSMENT: kubeflow/model-registry")
print("=" * 80)
print()

repo_path = Path("/tmp/model-registry")

if not repo_path.exists():
    print("âŒ Repository not found at /tmp/model-registry")
    print("Please clone it first: git clone https://github.com/kubeflow/model-registry.git /tmp/model-registry")
    sys.exit(1)

# Test 1: Repository Detection
print("ğŸ“¦ REPOSITORY DETECTION")
print("-" * 80)

from agentready.services.repository_service import RepositoryService

repo_service = RepositoryService()
repo = repo_service.extract_repo_info(str(repo_path))

print(f"âœ“ Name: {repo.name}")
print(f"âœ“ URL: {repo.repo_url}")
print(f"âœ“ Primary Language: {repo.primary_language}")
print()

# Get size info
size_info = repo_service.get_repo_size_info(repo_path)
print(f"âœ“ Source Files: {size_info['file_count']}")
print(f"âœ“ Lines of Code: {size_info['line_count']:,}")
print()

# Test 2: Manual Analysis of What Assessors Would Find
print("ğŸ” QUALITY ANALYSIS")
print("-" * 80)
print()

# Test Coverage Assessor - USING ACTUAL ASSESSOR
print("1ï¸âƒ£  TEST COVERAGE ASSESSOR (Multi-Language)")
print("-" * 80)

try:
    from agentready.assessors.quality.test_coverage import TestCoverageAssessor
    from agentready.models.repository import Repository as AssessorRepo
    
    assessor_repo = AssessorRepo(
        url=repo.repo_url,
        path=str(repo_path),
        name=repo.name,
        languages=[repo.primary_language] if repo.primary_language else [],
        metadata={}
    )
    
    assessor = TestCoverageAssessor()
    finding = assessor.assess(assessor_repo)
    
    coverage_score = finding.score if finding.score else 0
    
    print(f"âœ… Real Assessor Results:")
    print(f"   Score: {coverage_score:.1f}/100")
    print(f"   Status: {finding.status}")
    print(f"   Evidence: {finding.evidence}")
    print()
    
except Exception as e:
    print(f"âŒ Assessor error: {e}")
    # Fallback: manually count with new patterns
    test_count = 0
    source_count = 0
    
    # Count Go tests
    go_tests = len(list(repo_path.glob("**/*_test.go")))
    # Count TS/JS tests
    ts_tests = len(list(repo_path.glob("**/*.test.ts"))) + len(list(repo_path.glob("**/*.spec.ts")))
    # Count Python tests
    py_tests = len(list(repo_path.glob("**/test_*.py")))
    # Count E2E
    e2e_tests = len(list(repo_path.glob("**/e2e/**/*"))) + len(list(repo_path.glob("**/cypress/**/*")))
    
    test_count = go_tests + ts_tests + py_tests
    
    print(f"   Manual Count (Multi-Language):")
    print(f"   â€¢ Go tests: {go_tests}")
    print(f"   â€¢ TypeScript/JS tests: {ts_tests}")
    print(f"   â€¢ Python tests: {py_tests}")
    print(f"   â€¢ E2E test files: {e2e_tests}")
    print(f"   â€¢ Total: {test_count} test files")
    
    # Weighted estimate
    weighted = go_tests + ts_tests + py_tests + (e2e_tests * 2)
    ratio = weighted / size_info['file_count']
    estimated_coverage = min(100, ratio * 50)
    coverage_score = (estimated_coverage / 80) * 100
    
    print(f"   ğŸ“Š Estimated Coverage: {estimated_coverage:.1f}%")
    print(f"   ğŸ“Š Test Coverage Score: {coverage_score:.1f}/100")
    print()

# Integration Tests Assessor
print("2ï¸âƒ£  INTEGRATION TESTS ASSESSOR")
print("-" * 80)

integration_patterns = [
    "**/test_integration*.py",
    "**/integration_test*.py",
    "**/tests/integration/**/*.py",
    "**/e2e/**/*.py",
]
integration_files = []
for pattern in integration_patterns:
    integration_files.extend(list(repo_path.glob(pattern)))

print(f"Integration Test Files: {len(integration_files)}")
if integration_files:
    for test_file in integration_files[:3]:
        print(f"  â€¢ {test_file.relative_to(repo_path)}")
    if len(integration_files) > 3:
        print(f"  ... and {len(integration_files) - 3} more")

# Check for testcontainers or docker-compose.test
test_containers = list(repo_path.glob("**/*testcontainer*")) + list(repo_path.glob("**/docker-compose*test*"))
print(f"Test Containers: {'âœ“ Found' if test_containers else 'âœ— Not found'}")

integration_score = min(100, (len(integration_files) / 10) * 100)
if test_containers:
    integration_score = min(100, integration_score + 10)

print(f"ğŸ“Š Integration Tests Score: {integration_score:.1f}/100")
print()

# Documentation Standards Assessor
print("3ï¸âƒ£  DOCUMENTATION STANDARDS ASSESSOR")
print("-" * 80)

# Check README
readme_path = repo_path / "README.md"
readme_exists = readme_path.exists()
readme_score = 0

if readme_exists:
    content = readme_path.read_text()
    readme_score = 20  # Base score
    
    sections = {
        "Installation": ["install", "setup", "getting started"],
        "Usage": ["usage", "example", "quick start"],
        "Contributing": ["contribut", "development"],
        "License": ["license"],
    }
    
    found_sections = []
    for section, keywords in sections.items():
        if any(kw in content.lower() for kw in keywords):
            readme_score += 20
            found_sections.append(section)
    
    print(f"README.md: âœ“ Present")
    print(f"  Sections found: {', '.join(found_sections)}")
    print(f"  README Score: {readme_score}/100")
else:
    print(f"README.md: âœ— Missing")

# Check for docs directory
docs_dir = repo_path / "docs"
doc_files = list(docs_dir.glob("**/*.md")) if docs_dir.exists() else []
print(f"Documentation Directory: {'âœ“' if docs_dir.exists() else 'âœ—'}")
print(f"  Documentation files: {len(doc_files)}")

# Check architecture docs
arch_docs = [
    "ARCHITECTURE.md",
    "DESIGN.md",
    "docs/architecture.md",
    "docs/design.md",
]
arch_score = 0
for doc in arch_docs:
    if (repo_path / doc).exists():
        arch_score = 100
        print(f"  Architecture docs: âœ“ {doc}")
        break
else:
    if len(doc_files) > 5:
        arch_score = min(100, len(doc_files) * 20)
        print(f"  Architecture docs: ~ {len(doc_files)} doc files in docs/")

# Check docstrings in Python files (sample)
py_files = list(repo_path.glob("**/*.py"))[:50]
py_with_docstrings = 0
for py_file in py_files:
    try:
        content = py_file.read_text()
        if '"""' in content or "'''" in content:
            py_with_docstrings += 1
    except Exception:
        pass

docstring_coverage = (py_with_docstrings / len(py_files)) * 100 if py_files else 50
print(f"  Docstring coverage (sample): {docstring_coverage:.1f}%")

doc_overall = (readme_score * 0.4 + docstring_coverage * 0.4 + arch_score * 0.2)
print(f"ğŸ“Š Documentation Standards Score: {doc_overall:.1f}/100")
print()

# Ecosystem Tools Assessor
print("4ï¸âƒ£  ECOSYSTEM TOOLS ASSESSOR")
print("-" * 80)

tools_found = {}

# CI/CD
ci_indicators = [
    ".github/workflows",
    ".gitlab-ci.yml",
    ".travis.yml",
    "Jenkinsfile",
]
ci_found = False
for indicator in ci_indicators:
    path = repo_path / indicator
    if path.exists():
        ci_found = True
        workflows = list(path.glob("*.yml")) if path.is_dir() else [path]
        print(f"âœ“ CI/CD: {indicator} ({len(workflows) if path.is_dir() else 1} workflow(s))")
        tools_found['ci_cd'] = True
        break

if not ci_found:
    print("âœ— CI/CD: Not found")
    tools_found['ci_cd'] = False

# Code coverage
coverage_indicators = [".coveragerc", ".coverage", "codecov.yml", ".codecov.yml"]
coverage_found = any((repo_path / ind).exists() for ind in coverage_indicators)
print(f"{'âœ“' if coverage_found else 'âœ—'} Code Coverage: {'Found' if coverage_found else 'Not configured'}")
tools_found['code_coverage'] = coverage_found

# Security scanning
security_indicators = [".snyk", "snyk.yml", ".github/dependabot.yml"]
security_found = any((repo_path / ind).exists() for ind in security_indicators)
if not security_found:
    # Check in workflows
    gh_workflows = repo_path / ".github" / "workflows"
    if gh_workflows.exists():
        for wf in gh_workflows.glob("*.yml"):
            try:
                content = wf.read_text().lower()
                if any(tool in content for tool in ["snyk", "dependabot", "codeql", "trivy"]):
                    security_found = True
                    break
            except Exception:
                pass

print(f"{'âœ“' if security_found else 'âœ—'} Security Scanning: {'Found' if security_found else 'Not configured'}")
tools_found['security_scanning'] = security_found

# Linting
linting_indicators = [".eslintrc", ".pylintrc", ".flake8", "pyproject.toml", ".golangci.yml"]
linting_found = any((repo_path / ind).exists() for ind in linting_indicators)
if not linting_found:
    for pattern in ["**/.eslintrc*", "**/.pylintrc*"]:
        if list(repo_path.glob(pattern)):
            linting_found = True
            break

print(f"{'âœ“' if linting_found else 'âœ—'} Linting: {'Configured' if linting_found else 'Not configured'}")
tools_found['linting'] = linting_found

# Dependency management
dep_files = ["go.mod", "go.sum", "package-lock.json", "yarn.lock", "requirements.txt", "Pipfile.lock"]
dep_found = any((repo_path / df).exists() for df in dep_files)
found_deps = [df for df in dep_files if (repo_path / df).exists()]
print(f"{'âœ“' if dep_found else 'âœ—'} Dependency Management: {', '.join(found_deps) if found_deps else 'Not found'}")
tools_found['dependency_management'] = dep_found

# Pre-commit
pre_commit = (repo_path / ".pre-commit-config.yaml").exists()
print(f"{'âœ“' if pre_commit else 'âœ—'} Pre-commit Hooks: {'Configured' if pre_commit else 'Not configured'}")
tools_found['pre_commit_hooks'] = pre_commit

# Calculate ecosystem score
weights = {
    'ci_cd': 30,
    'code_coverage': 20,
    'security_scanning': 20,
    'linting': 15,
    'dependency_management': 10,
    'pre_commit_hooks': 5,
}
ecosystem_score = sum(weights[tool] for tool, present in tools_found.items() if present)
print(f"ğŸ“Š Ecosystem Tools Score: {ecosystem_score:.1f}/100")
print()

# Overall Score Calculation
print("=" * 80)
print("ğŸ“Š OVERALL QUALITY ASSESSMENT")
print("=" * 80)
print()

from agentready.services.quality_scorer import QualityScorerService

scorer = QualityScorerService()

# Create mock results with our calculated scores
from agentready.models.assessor_result import AssessorResult

mock_results = [
    AssessorResult(
        assessment_id="test",
        assessor_name="quality_test_coverage",
        score=coverage_score,
        metrics={
            "line_coverage": estimated_coverage,
            "test_count": 157,  # From manual count above
            "test_to_code_ratio": ratio,
        },
        status="success",
        executed_at=datetime.utcnow()
    ),
    AssessorResult(
        assessment_id="test",
        assessor_name="quality_integration_tests",
        score=integration_score,
        metrics={
            "integration_test_count": len(integration_files),
            "test_containers": bool(test_containers),
        },
        status="success",
        executed_at=datetime.utcnow()
    ),
    AssessorResult(
        assessment_id="test",
        assessor_name="quality_documentation_standards",
        score=doc_overall,
        metrics={
            "readme_score": readme_score,
            "docstring_coverage": docstring_coverage,
            "architecture_docs_present": arch_score > 0,
        },
        status="success",
        executed_at=datetime.utcnow()
    ),
    AssessorResult(
        assessment_id="test",
        assessor_name="quality_ecosystem_tools",
        score=ecosystem_score,
        metrics=tools_found,
        status="success",
        executed_at=datetime.utcnow()
    ),
]

overall_score = scorer.calculate_overall_score(mock_results)
performance_tier = scorer.get_performance_tier(overall_score)

print(f"Overall Score: {overall_score:.1f}/100")
print(f"Performance Tier: ğŸ† {performance_tier}")
print()

print("Individual Scores:")
for result in mock_results:
    name = result.assessor_name.replace("quality_", "").replace("_", " ").title()
    emoji = "âœ…" if result.score >= 70 else "âš ï¸" if result.score >= 50 else "âŒ"
    print(f"  {emoji} {name}: {result.score:.1f}/100")

print()

# Identify weak areas
weak_areas = scorer.identify_weakest_areas(mock_results, threshold=70)
if weak_areas:
    print("âš ï¸  Areas Needing Improvement:")
    for area in weak_areas:
        name = area.replace("quality_", "").replace("_", " ").title()
        print(f"  â€¢ {name}")
    print()

# Generate recommendations
print("ğŸ’¡ KEY RECOMMENDATIONS")
print("-" * 80)

recommendations = []

if coverage_score < 70:
    recommendations.append(
        f"ğŸ”´ HIGH: Increase test coverage to 80%+ (currently ~{estimated_coverage:.0f}%)"
    )

if integration_score < 70:
    recommendations.append(
        f"ğŸŸ¡ MEDIUM: Add more integration tests (found {len(integration_files)}, recommend 10+)"
    )

if doc_overall < 70:
    recommendations.append(
        "ğŸŸ¡ MEDIUM: Improve documentation (add missing README sections or docstrings)"
    )

if ecosystem_score < 90:
    missing = [tool.replace("_", " ").title() for tool, present in tools_found.items() if not present]
    if missing:
        recommendations.append(
            f"ğŸŸ¡ MEDIUM: Add ecosystem tools: {', '.join(missing)}"
        )

if not recommendations:
    recommendations.append("âœ… Excellent quality! All metrics above thresholds. Keep maintaining current standards.")

for rec in recommendations:
    print(f"{rec}")

print()

# Summary
print("=" * 80)
print("ğŸ“‹ ASSESSMENT SUMMARY")
print("=" * 80)
print()
print(f"Repository: kubeflow/model-registry")
print(f"Primary Language: {repo.primary_language}")
print(f"Files Analyzed: {size_info['file_count']}")
print(f"Lines of Code: {size_info['line_count']:,}")
print()
print(f"âœ¨ Overall Quality Score: {overall_score:.1f}/100")
print(f"ğŸ† Performance Tier: {performance_tier}")
print()

tier_descriptions = {
    "Elite": "Top 10% - Exceptional quality engineering practices",
    "High": "Top 35% - Strong quality foundation with minor gaps",
    "Medium": "Middle 40% - Acceptable baseline, room for improvement",
    "Low": "Bottom 25% - Significant quality gaps to address",
}

print(f"What this means: {tier_descriptions[performance_tier]}")
print()

print("=" * 80)
print("âœ… ASSESSMENT COMPLETE")
print("=" * 80)
print()
print("This is a simulated assessment based on file analysis.")
print("For full assessment with actual coverage parsing, install dependencies:")
print()
print("  cd /Users/ykrimerm/hackthon1/ship-shape")
print("  python3 -m venv .venv")
print("  source .venv/bin/activate")
print("  pip install -e '.[dev]'")
print("  agentready assess-quality /tmp/model-registry")
print()
